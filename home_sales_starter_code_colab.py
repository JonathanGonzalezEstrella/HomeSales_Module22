# -*- coding: utf-8 -*-
"""Home_Sales_starter_code_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O0JWj8ugPUGnsUfYAHIhE8zuoiO4XDJk
"""

import os
# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version
# For example:
# spark_version = 'spark-3.5.1'
spark_version = 'spark-3.5.1'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz
!tar xf $SPARK_VERSION-bin-hadoop3.tgz
!pip install -q findspark

# Set Environment Variables
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop3"

# Start a SparkSession
import findspark
findspark.init()

# Import packages
from pyspark.sql import SparkSession
import time

# Create a SparkSession
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()

# 1. Read in the AWS S3 bucket into a DataFrame.
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"

# Download the CSV file from the URL
!wget -q {url} -O /tmp/home_sales_revised.csv

spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get("home_sales_revised.csv"), sep=",", header=True, ignoreLeadingWhiteSpace=True)

# Show DataFrame
df.show()

# 2. Create a temporary view of the DataFrame.

df.createOrReplaceTempView("home_sales")

# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?
# Calculate the average price for a four-bedroom house sold per year
average_price_query = """
SELECT
    year(date) as year,
    round(avg(price), 2) as average_price
FROM
    home_sales
WHERE
    bedrooms = 4
GROUP BY
    year(date)
ORDER BY
    year
"""

# Execute the query
average_price_df = spark.sql(average_price_query)

# Show the results
average_price_df.show()

# Calculate the average price of a home for each year the home was built,
# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places
average_price_built_year_query = """
SELECT
    date_built,
    round(avg(price), 2) as average_price
FROM
    home_sales
WHERE
    bedrooms = 3 AND bathrooms = 3
GROUP BY
    date_built
ORDER BY
    date_built
"""

# Execute the query
average_price_built_year_df = spark.sql(average_price_built_year_query)

# Show the results
average_price_built_year_df.show()

# 5. What is the average price of a home for each year the home was built,
# that have 3 bedrooms, 3 bathrooms, with two floors,
# and are greater than or equal to 2,000 square feet, rounded to two decimal places?

# Calculate the average price of a home for each year the home was built,
# that have 3 bedrooms, 3 bathrooms, with two floors,
# and are greater than or equal to 2,000 square feet, rounded to two decimal places
average_price_specific_criteria_query = """
SELECT
    date_built,
    round(avg(price), 2) as average_price
FROM
    home_sales
WHERE
    bedrooms = 3
    AND bathrooms = 3
    AND floors = 2
    AND sqft_living >= 2000
GROUP BY
    date_built
ORDER BY
    date_built
"""

# Execute the query
average_price_specific_criteria_df = spark.sql(average_price_specific_criteria_query)

# Show the results
average_price_specific_criteria_df.show()

# 6. What is the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000? Order by descending view rating.
# Although this is a small dataset, determine the run time for this query.

import time

# Start the timer
start_time = time.time()

# Calculate the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000
average_price_view_query = """
SELECT
    view,
    round(avg(price), 2) as average_price
FROM
    home_sales
GROUP BY
    view
HAVING
    avg(price) >= 350000
ORDER BY
    view DESC
"""

# Execute the query
average_price_view_df = spark.sql(average_price_view_query)

# Show the results
average_price_view_df.show()

# End the timer and calculate the run time
end_time = time.time()
run_time = end_time - start_time

print(f"Run time for the query: {run_time:.2f} seconds")
start_time = time.time()



print("--- %s seconds ---" % (time.time() - start_time))

# 7. Cache the the temporary table home_sales.

# Cache the temporary table home_sales
spark.catalog.cacheTable("home_sales")

# Confirm the table is cached by showing the list of cached tables
spark.catalog.isCached("home_sales")

# 8. Check if the table is cached.
spark.catalog.isCached('home_sales')

# Check if the table home_sales is cached
is_cached = spark.catalog.isCached('home_sales')

# Print the result
print(f"Is 'home_sales' table cached? {is_cached}")

# 9. Using the cached data, run the last query above, that calculates
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000.
# Determine the runtime and compare it to the uncached runtime.

# Ensure the home_sales table is cached
spark.catalog.cacheTable("home_sales")

# Confirm the table is cached
is_cached = spark.catalog.isCached('home_sales')
print(f"Is 'home_sales' table cached? {is_cached}")

# Start the timer for the cached query
start_time_cached = time.time()

# Calculate the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000 using cached data
average_price_view_query_cached = """
SELECT
    view,
    round(avg(price), 2) as average_price
FROM
    home_sales
GROUP BY
    view
HAVING
    avg(price) >= 350000
ORDER BY
    view DESC
"""

# Execute the query using cached data
average_price_view_df_cached = spark.sql(average_price_view_query_cached)

# Show the results
average_price_view_df_cached.show()

# End the timer for the cached query and calculate the runtime
end_time_cached = time.time()
run_time_cached = end_time_cached - start_time_cached

print(f"Run time for the cached query: {run_time_cached:.2f} seconds")

# To compare, let's also measure the runtime without caching (if not already measured)
# Start the timer for the uncached query
start_time_uncached = time.time()

# Execute the uncached query again for comparison
average_price_view_df_uncached = spark.sql(average_price_view_query)

# Show the results
average_price_view_df_uncached.show()

# End the timer for the uncached query and calculate the runtime
end_time_uncached = time.time()
run_time_uncached = end_time_uncached - start_time_uncached

print(f"Run time for the uncached query: {run_time_uncached:.2f} seconds")

# Print comparison
print(f"Run time comparison - Cached: {run_time_cached:.2f} seconds, Uncached: {run_time_uncached:.2f} seconds")
start_time = time.time()



print("--- %s seconds ---" % (time.time() - start_time))

# 10. Partition by the "date_built" field on the formatted parquet home sales data
# Write the DataFrame to a Parquet file with partitioning by 'date_built'
output_path = "/tmp/home_sales_partitioned"

df.write.partitionBy("date_built").parquet(output_path)

# Confirm the files are written and partitioned correctly
print(f"Data written to {output_path} partitioned by 'date_built'.")

# 11. Read the parquet formatted data.
# Specify the path to the Parquet data
parquet_path = "/tmp/home_sales_partitioned"

# Read the Parquet data
df_parquet = spark.read.parquet(parquet_path)

# Show the schema to confirm the data is loaded correctly
df_parquet.printSchema()

# Show some sample data to verify
df_parquet.show()

# 12. Create a temporary table for the parquet data.
# Specify the path to the Parquet data
parquet_path = "/tmp/home_sales_partitioned"

# Read the Parquet data
df_parquet = spark.read.parquet(parquet_path)

# Create a temporary view of the Parquet data
df_parquet.createOrReplaceTempView("home_sales_parquet")

# Confirm the temporary view is created
print("Temporary view 'home_sales_parquet' created.")

# 13. Using the parquet DataFrame, run the last query above, that calculates
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000.
# Determine the runtime and compare it to the cached runtime.

import time

# Ensure the Parquet data is loaded and a temporary view is created
parquet_path = "/tmp/home_sales_partitioned"
df_parquet = spark.read.parquet(parquet_path)
df_parquet.createOrReplaceTempView("home_sales_parquet")

# Confirm the temporary view is created
print("Temporary view 'home_sales_parquet' created.")

# Define the query
average_price_view_query = """
SELECT
    view,
    round(avg(price), 2) as average_price
FROM
    home_sales_parquet
GROUP BY
    view
HAVING
    avg(price) >= 350000
ORDER BY
    view DESC
"""

# Measure the runtime using the Parquet DataFrame
start_time_parquet = time.time()
average_price_view_df_parquet = spark.sql(average_price_view_query)
average_price_view_df_parquet.show()
end_time_parquet = time.time()
run_time_parquet = end_time_parquet - start_time_parquet

print(f"Run time for the query using Parquet data: {run_time_parquet:.2f} seconds")

# Compare with the cached runtime (assuming the cached runtime has already been measured and stored in run_time_cached)
print(f"Run time comparison - Parquet: {run_time_parquet:.2f} seconds, Cached: {run_time_cached:.2f} seconds")

start_time = time.time()



print("--- %s seconds ---" % (time.time() - start_time))

# 14. Uncache the home_sales temporary table.
# Uncache the temporary table home_sales
spark.catalog.uncacheTable("home_sales")

# Confirm the table is uncached
is_cached = spark.catalog.isCached("home_sales")
print(f"Is 'home_sales' table cached? {is_cached}")

# 15. Check if the home_sales is no longer cached

# Check if the home_sales table is no longer cached
is_cached = spark.catalog.isCached("home_sales")

# Print the result
print(f"Is 'home_sales' table cached? {is_cached}")

